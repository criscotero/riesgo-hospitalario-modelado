{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwdcNHRg-dF0"
   },
   "source": [
    "# Final Project - Hospitalization Prediction for Elderly People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxve20X42VBm",
    "outputId": "e90687c3-8a3e-4837-d6f4-eb2ae43667d2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_data \u001b[38;5;28;01mas\u001b[39;00m data, preprocessing, eda, feature_selection\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/AnyoneAI/ProyectoFinal/riesgo-hospitalario-modelado/src/feature_selection.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mselect_top_features_tree\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, target: \u001b[38;5;28mstr\u001b[39m, n_features: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from src import extract_data as data, preprocessing, eda, feature_selection\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *The Mexican Health and Aging Study* (**MHAS**) is a dataset of household surveys designed to collect information on the health, economic status, and quality of life of older adults.\n",
    "The survey was conducted over 5 time periods, technically known as **Waves**. \n",
    "In addition, there are three study subjects: the respondent (r), the spouse (s), and the household (H). For this study we will use the last wave and the respondent(r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the data for this project, you only need to execute the code below. This will download H_MHAS_c2.sas7bdat file inside the `dataset` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once, or if you need to rebuild the original data\n",
    "# df = data.download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('We have',df.shape[0],'subjects')\n",
    "# print('We have',df.shape[1],'features')\n",
    "# print('Head', df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have download the dataset, you only need to execute the code below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "The dataset `H_MHAS_c2.sas7bdat` has 26839 rows and 5241 features.\n",
    "\n",
    "All features are divided into the following sections.\n",
    "\n",
    "- SECTION A: DEMOGRAPHICS, IDENTIFIERS, AND WEIGHTS \n",
    "- SECTION B: HEALTH \n",
    "- SECTION C: HEALTH CARE UTILIZATION AND INSURANCE \n",
    "- SECTION D: COGNITION  \n",
    "- SECTION E: FINANCIAL AND HOUSING WEALTH \n",
    "- SECTION F: INCOME\n",
    "- SECTION G: FAMILY STRUCTURE \n",
    "- SECTION H: EMPLOYMENT HISTORY \n",
    "- SECTION I: RETIREMENT \n",
    "- SECTION J: PENSION \n",
    "- SECTION K: PHYSICAL MEASURES\n",
    "- SECTION L: ASSISTANCE AND CAREGIVING\n",
    "- SECTION M: STRESS \n",
    "- SECTION O: END OF LIFE PLANNING\n",
    "- SECTION Q: PSYCHOSOCIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical and Numerical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = df.select_dtypes(include=['object']).columns\n",
    "numerical_vars = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "print('Categorical features ' + str(len(categorical_vars)))\n",
    "print('Numerical features ' + str(len(numerical_vars)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will save the features with the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.save_categorical_features_with_values(df, 'features_with_values.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this study, we will focus on the Respondent\n",
    "* The Householder variable will be removed from data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split waves\n",
    "\n",
    "Our initial approach is to train our model using separate waves. We made this decision because the last three waves took place every two years and there is little data available to helps us bridge that gap, so a cross-sectional cut of the data makes sense as our best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_5_df = preprocessing.extract_wave_data(df, \"5\")\n",
    "\n",
    "print(f'Wave 5 dataframe has the following shape: {wave_5_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split-respondents'></a>\n",
    "### Split respondents\n",
    "\n",
    "Our initial approach is to train the model using only data from the respondents, as we believe it is the most relevant information to properly train our model; also, given that our MPV requires interaction with the people interested in receiving a hospitalization prediction, we deem it best to ask them questions abouth themselves rather than their spouse or household, as such information might not be available during their interaction with our MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_5_respondents_df = preprocessing.extract_respondent_data(wave_5_df)\n",
    "\n",
    "print(f'Wave 5 respondent-only dataframe has the following shape: {wave_5_respondents_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.save_categorical_features_with_values(wave_5_respondents_df, 'wave_5_features_with_values.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features: {wave_5_respondents_df.shape[1]}\")\n",
    "eda.plot_missing_values(wave_5_respondents_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of the target variable\n",
    "* The target variable belongs to *Section C: Health Care Utilization and Insurance* and is labeled **Medical Care Utilization: Hospital** *`rhosp1y`*\n",
    "\n",
    "* *rhosp1y* indicates whether the respondent reports at least one overnight hospital stay in the last 12 months. RHOSP1Y is coded as 0 if the respondent had no overnight hospital stays, and is coded as 1 if the respondent had at least one overnight hospital stay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values in target Variable rhosp1y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.plot_missing_percentage(wave_5_respondents_df, 'r5hosp1y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remove-missing-target'></a>\n",
    "### Remove missing values in target variable\n",
    "\n",
    "Our first step is to remove all rows containing missing values in our target variable. Why remove them instead of imputate them? Because this is our ground truth: We cannot alter it by somehow estimating missing values from the data. If we attempt to imputate our ground truth with other features, we'd be incorporating information about the data into the target variable, which could very likely lead us to overfit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_5_respondents_df = preprocessing.remove_missing_values(wave_5_respondents_df, 'r5hosp1y')\n",
    "\n",
    "print(f'Shape: {wave_5_respondents_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.plot_age_histogram(wave_5_respondents_df, 'r5agey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the data by elderly people only(50+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = eda.filter_age(wave_5_respondents_df, 'r5agey')\n",
    "print(f'Shape: {df_filtered.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='drop-high-rate-missing-values-columns'></a>\n",
    "\n",
    "### Drop columns with a high rate of missing values\n",
    "\n",
    "We have decided to drop columns with a high missing values ratio (>0.7). A column with such a high proportion of missing values hints at survey unreliability and it doesn't make much sense to imputate missing values when their proportion is higher than existing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_drop = preprocessing.missing_value_ratio(wave_5_respondents_df, 0.7)\n",
    "\n",
    "# Drop the columns with specified missing values ratio\n",
    "wave_5_respondents_df = wave_5_respondents_df.drop(columns=variables_to_drop)\n",
    "\n",
    "# Verify columns were droped. Starting column count is 469\n",
    "print(f'New column count: {wave_5_respondents_df.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.save_categorical_features_with_values(wave_5_respondents_df, 'wave_5_features_out_missing_value.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based models like Random Forest or XGBoost provide feature importance scores. You can use them to select the top 20-30 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = eda.select_top_features_tree(wave_5_respondents_df, target=\"your_target_variable\", n_features=25)\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information measures how much information a feature shares with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_mutual = eda.select_top_features_mi(wave_5_respondents_df, target=\"your_target_variable\", n_features=25)\n",
    "print(selected_features_mutual)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
