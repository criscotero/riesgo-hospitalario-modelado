{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodological overview and notebook's sections\n",
    "\n",
    "We intend to leverage the fact that our dataset has several waves to balance our target variable without adding synthetic data or using random oversampling. Given that our dataset fundamentally consists of categorical data, adding synthetic data through methods like SMOTE isn't particularly useful to us, and might even be detrimental, because it might introduce unrealistic categorical combinations that hinder the model's ability to generalize over real data; and eventhough there's a version for categorical data (SMOTE-N), synthetic noise will still be introduced into the data.\n",
    "\n",
    "In order to achieve this, we first need to reshape our data so that we only have one hospitalization variable, instead of having a hospitalization variable per wave. We will also use both respondent ('r'-prefixed) and spouse ('s'-prefixed). This means that our number of rows will grow by a factor of *2w*, where *w* is the number of waves. \n",
    "\n",
    "Our strategy consists of two approaches. In our first approach, we'll use waves 3, 4 and 5, take the entirety of wave 5 and rows where 'hospitalized'= 1 in waves 3 and 4 to enhance wave 5. This will result in a more balanced dataset without undersampling, and notably, we won't add noise with synthetic data: we will balance our dataset organically. We chose these 3 waves because they are evenly spread in time (2012, 2015, 2018) and because we want to establish baseline metrics to determine the impact of additional waves.\n",
    "\n",
    "In our second approach, we will use waves 1, 2, 3 and 4 to balance wave 5. This should result in an even smaller gap between the majority and minority classes.\n",
    "\n",
    "\n",
    "\n",
    "<a id='sections'></a>\n",
    "#### ðŸ“Œ [Sections](#sections) \n",
    "- [Reshaping the data](#data-reshaping)  \n",
    "- [Removal of proxies, respondents under 50 and missing values in target](#removals)   \n",
    "- [Key distributions](#key_dists)  \n",
    "- [Destringify values of categorical variables](#destringify-cat-vals)  \n",
    "- [First approach: waves 3, 4 and 5](#approach-1) \n",
    "  - [Baseline: no imputations](#baseline-no-imputations) \n",
    "  - [Baseline: imputations](#baseline-imputations)  \n",
    "- [Second approach: all waves](#approach-2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "\n",
    "from src import features_registry, data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw_df has shape (26839, 5241)\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/H_MHAS_c2.dta'\n",
    "raw_df = pd.read_stata(file_path)\n",
    "\n",
    "print(f\"Raw_df has shape {raw_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-reshaping'></a>\n",
    "### Reshaping the data \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataframe shape: (268390, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5268"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get selected features \n",
    "selected_features_suffixes = features_registry.selected_features\n",
    "\n",
    "target_suffix = 'hosp1y'\n",
    "\n",
    "# Get prefixed target variables\n",
    "waves = range(1, 6)\n",
    "\n",
    "target_variables = data_utils.join_prefix_suffix(waves, target_suffix)\n",
    "# Get stacked dfs\n",
    "\n",
    "stacked_df = data_utils.stack_df(raw_df, target_variables, selected_features_suffixes, rural=False)\n",
    "\n",
    "\n",
    "# Verify rows multiplied to the corresponding factor\n",
    "print(f\"New dataframe shape: {stacked_df.shape}\")\n",
    "\n",
    "\n",
    "# Release raw_df from memory\n",
    "del raw_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='removals'></a>\n",
    "### Removal of proxies, respondents under 50 and missing values in target\n",
    "\n",
    "<small>[Back to top](#sections)</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy\n",
      "NaN            143790\n",
      "0.Not proxy    115618\n",
      "1.Proxy          8982\n",
      "Name: count, dtype: int64\n",
      "proxy\n",
      "0.Not proxy    115618\n",
      "Name: count, dtype: int64\n",
      "stacked_df now has 115618 rows\n"
     ]
    }
   ],
   "source": [
    "#  <---- Inspect unique values for 'proxy' and only keep non-proxy respondents ---->\n",
    "\n",
    "print(stacked_df['proxy'].value_counts(dropna=False))\n",
    "stacked_df.drop(index=stacked_df[stacked_df['proxy'] != '0.Not proxy'].index, inplace=True)\n",
    "print(stacked_df['proxy'].value_counts(dropna=False))\n",
    "print(f\"stacked_df now has {stacked_df.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospitalized\n",
      "0.No     102518\n",
      "1.Yes     12915\n",
      "NaN         185\n",
      "Name: count, dtype: int64\n",
      "hospitalized\n",
      "0.No     102518\n",
      "1.Yes     12915\n",
      "Name: count, dtype: int64\n",
      "stacked_df now has 115433 rows\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect missing values for 'hospitalized' and remove them ---->\n",
    "\n",
    "print(stacked_df['hospitalized'].value_counts(dropna=False))\n",
    "stacked_df.dropna(subset=['hospitalized'], inplace=True)\n",
    "print(stacked_df['hospitalized'].value_counts(dropna=False))\n",
    "print(f\"stacked_df now has {stacked_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 16.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 110.0, 112.0]\n",
      "[50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 110.0, 112.0]\n",
      "stacked_df now has 106537 rows\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect respondents under 50 and remove them ---->\n",
    "\n",
    "unique_agey = sorted(stacked_df['agey'].unique(), key=lambda x: (not pd.isna(x), x))\n",
    "print(unique_agey)\n",
    "stacked_df.dropna(subset=['agey'], inplace=True)  # Remove NaN values\n",
    "stacked_df.drop(index=stacked_df[stacked_df['agey'] < 50].index, inplace=True)  # Remove values < 50\n",
    "unique_agey = sorted(stacked_df['agey'].unique(), key=lambda x: (not pd.isna(x), x))\n",
    "print(unique_agey)\n",
    "\n",
    "print(f\"stacked_df now has {stacked_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='key-dists'></a>\n",
    "### Key distributions \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospitalized prportions across all waves: hospitalized\n",
      "0.No     0.886368\n",
      "1.Yes    0.113632\n",
      "Name: proportion, dtype: float64\n",
      "Hospitalized proportions per wave: hospitalized      0.No     1.Yes\n",
      "wave                            \n",
      "1             0.907720  0.092280\n",
      "2             0.893766  0.106234\n",
      "3             0.895553  0.104447\n",
      "4             0.869383  0.130617\n",
      "5             0.866445  0.133555\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect target imbalances ---->\n",
    "\n",
    "hospitalized_proportions = stacked_df['hospitalized'].value_counts(normalize=True)\n",
    "print(f'Hospitalized prportions across all waves: {hospitalized_proportions}')\n",
    "\n",
    "hospitalized_proportions_by_wave = (\n",
    "    stacked_df.groupby('wave')['hospitalized']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "print(f'Hospitalized proportions by wave: {hospitalized_proportions_by_wave}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender proportions by wave: gender     1.Man   2.Woman\n",
      "wave                      \n",
      "1       0.496927  0.503073\n",
      "2       0.474486  0.525514\n",
      "3       0.463969  0.536031\n",
      "4       0.461626  0.538374\n",
      "5       0.453300  0.546700\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect gender proportions across waves ---->\n",
    "\n",
    "gender_proportions_by_wave = (\n",
    "    stacked_df.groupby('wave')['gender']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "print(f'Gender proportions by wave: {gender_proportions_by_wave}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender                1.Man   2.Woman\n",
      "wave hospitalized                    \n",
      "1    0.No          0.501693  0.498307\n",
      "     1.Yes         0.450052  0.549948\n",
      "2    0.No          0.479895  0.520105\n",
      "     1.Yes         0.428983  0.571017\n",
      "3    0.No          0.470340  0.529660\n",
      "     1.Yes         0.409340  0.590660\n",
      "4    0.No          0.466974  0.533026\n",
      "     1.Yes         0.426029  0.573971\n",
      "5    0.No          0.456380  0.543620\n",
      "     1.Yes         0.433322  0.566678\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect gender proportions by wave ---->\n",
    "\n",
    "hospitalized_gender_proportions_by_wave = (\n",
    "    stacked_df.groupby(['wave', 'hospitalized'])['gender']\n",
    "    .value_counts(normalize=True)\n",
    "    .unstack()\n",
    ")\n",
    "print(hospitalized_gender_proportions_by_wave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                count       mean       std   min   25%   50%   75%    max\n",
      "wave gender                                                              \n",
      "1    1.Man    10350.0  61.853623  9.236059  50.0  54.0  60.0  68.0  105.0\n",
      "     2.Woman  10478.0  61.106509  8.929668  50.0  54.0  59.0  67.0   99.0\n",
      "2    1.Man     9308.0  63.969059  8.982577  50.0  57.0  62.0  70.0  107.0\n",
      "     2.Woman  10309.0  62.129887  8.997397  50.0  55.0  60.0  68.0  103.0\n",
      "3    1.Man    10559.0  65.247372  9.190321  50.0  58.0  64.0  71.0  110.0\n",
      "     2.Woman  12199.0  63.440200  9.068684  50.0  56.0  62.0  69.0  112.0\n",
      "4    1.Man    10129.0  66.935235  9.090652  50.0  60.0  67.0  73.0  101.0\n",
      "     2.Woman  11813.0  64.908321  9.162998  50.0  58.0  64.0  71.0  100.0\n",
      "5    1.Man     9697.0  66.570279  9.920810  50.0  58.0  67.0  74.0  101.0\n",
      "     2.Woman  11695.0  64.546131  9.787528  50.0  56.0  64.0  71.0  102.0\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect age distributions by wave and gender ---->\n",
    "\n",
    "age_distribution_by_gender_wave = stacked_df.groupby(['wave', 'gender'])['agey'].describe()\n",
    "print(age_distribution_by_gender_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='destringify-cat-vals'></a>\n",
    "##### Destringify values of categorical variables\n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"hospitalized\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.No\",\n",
      "            \"1.Yes\"\n",
      "        ]\n",
      "    },\n",
      "    \"gender\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"2.Woman\",\n",
      "            \"1.Man\"\n",
      "        ]\n",
      "    },\n",
      "    \"shlt\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"4.Fair\",\n",
      "            \"5.Poor\",\n",
      "            \"2.Very good\",\n",
      "            \"3.Good\",\n",
      "            \"1.Excellent\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"hltc\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"4.Somewhat worse\",\n",
      "            \"3.More or less the same\",\n",
      "            \"2.Somewhat better\",\n",
      "            \"5.Much worse\",\n",
      "            \"1.Much better\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"mobilseva\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.No\",\n",
      "            \"1.Yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"diabe\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"hrtatte\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"stroke\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"breath_m\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"doctor1y\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"1.Yes\",\n",
      "            \"0.No\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"cholst\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"1.Yes\",\n",
      "            \"0.No\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"breast\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"1.Yes\",\n",
      "            \"nan\",\n",
      "            \"0.No\"\n",
      "        ]\n",
      "    },\n",
      "    \"mammog\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.No\",\n",
      "            \"nan\",\n",
      "            \"1.Yes\"\n",
      "        ]\n",
      "    },\n",
      "    \"papsm\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"1.Yes\",\n",
      "            \"nan\",\n",
      "            \"0.No\"\n",
      "        ]\n",
      "    },\n",
      "    \"prost\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"nan\",\n",
      "            \"0.No\",\n",
      "            \"1.Yes\"\n",
      "        ]\n",
      "    },\n",
      "    \"vigact\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.No\",\n",
      "            \"1.Yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"smoken\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.No\",\n",
      "            \"1.Yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"painfr\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"fatigue\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"swell\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"hibpe\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"cancre\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"respe\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"rxresp\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"rxhibp\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"rxdiab\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"rxhrtat\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"rxstrok\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.no\",\n",
      "            \"1.yes\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"work\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.Not working for pay\",\n",
      "            \"1.Working for pay\",\n",
      "            \"nan\"\n",
      "        ]\n",
      "    },\n",
      "    \"proxy\": {\n",
      "        \"Type\": \"object\",\n",
      "        \"Unique values\": [\n",
      "            \"0.Not proxy\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <---- Inspect values of categorical variables ---->\n",
    "\n",
    "categorical_columns = stacked_df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "column_summary_dict = {\n",
    "    col: {\n",
    "        'Type': stacked_df[col].dtype.name,\n",
    "        'Unique values': stacked_df[col].astype(str).unique().tolist()\n",
    "    }\n",
    "    for col in categorical_columns    \n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(column_summary_dict, indent=4))\n",
    "\n",
    "\n",
    "del categorical_columns\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"hospitalized\": {\n",
      "        \"0.No\": 0.0,\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"gender\": {\n",
      "        \"2.Woman\": 2.0,\n",
      "        \"1.Man\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"shlt\": {\n",
      "        \"4.Fair\": 4.0,\n",
      "        \"5.Poor\": 5.0,\n",
      "        \"2.Very good\": 2.0,\n",
      "        \"3.Good\": 3.0,\n",
      "        \"1.Excellent\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"hltc\": {\n",
      "        \"4.Somewhat worse\": 4.0,\n",
      "        \"3.More or less the same\": 3.0,\n",
      "        \"2.Somewhat better\": 2.0,\n",
      "        \"5.Much worse\": 5.0,\n",
      "        \"1.Much better\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"mobilseva\": {\n",
      "        \"0.No\": 0.0,\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"diabe\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"hrtatte\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"stroke\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"breath_m\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"doctor1y\": {\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"0.No\": 0.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"cholst\": {\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"0.No\": 0.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"breast\": {\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN,\n",
      "        \"0.No\": 0.0\n",
      "    },\n",
      "    \"mammog\": {\n",
      "        \"0.No\": 0.0,\n",
      "        \"nan\": NaN,\n",
      "        \"1.Yes\": 1.0\n",
      "    },\n",
      "    \"papsm\": {\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN,\n",
      "        \"0.No\": 0.0\n",
      "    },\n",
      "    \"prost\": {\n",
      "        \"nan\": NaN,\n",
      "        \"0.No\": 0.0,\n",
      "        \"1.Yes\": 1.0\n",
      "    },\n",
      "    \"vigact\": {\n",
      "        \"0.No\": 0.0,\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"smoken\": {\n",
      "        \"0.No\": 0.0,\n",
      "        \"1.Yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"painfr\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"fatigue\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"swell\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"hibpe\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"cancre\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"respe\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"rxresp\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"rxhibp\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"rxdiab\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"rxhrtat\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"rxstrok\": {\n",
      "        \"0.no\": 0.0,\n",
      "        \"1.yes\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"work\": {\n",
      "        \"0.Not working for pay\": 0.0,\n",
      "        \"1.Working for pay\": 1.0,\n",
      "        \"nan\": NaN\n",
      "    },\n",
      "    \"proxy\": {\n",
      "        \"0.Not proxy\": 0.0,\n",
      "        \"nan\": NaN\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# <---- Extract numeric element of string and generate a mapping ---->\n",
    "\n",
    "# Function to extract numbers from categorical values (like \"1.Yes\" â†’ 1)\n",
    "def extract_number(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.search(r'\\d+', value)  # Find the first number\n",
    "        return float(match.group()) if match else value  # Return the number if found\n",
    "    return float(value) if isinstance(value, int) else value  # Keep NaN unchanged\n",
    "\n",
    "# Step 1: Generate mappings\n",
    "mappings = {}\n",
    "\n",
    "for col, info in column_summary_dict.items():\n",
    "    unique_values = info[\"Unique values\"]\n",
    "    \n",
    "    # Convert values to numbers if possible\n",
    "    mapping = {val: extract_number(val) for val in unique_values}\n",
    "    \n",
    "    # Ensure NaN is preserved correctly\n",
    "    mapping[\"nan\"] = np.nan  \n",
    "    \n",
    "    # Store mapping\n",
    "    mappings[col] = mapping\n",
    "\n",
    "import json\n",
    "print(json.dumps(mappings, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <---- Apply mappings while preserving 'category' type of variables ---->\n",
    "for col, mapping in mappings.items():\n",
    "    if col in stacked_df.columns:\n",
    "        stacked_df[col] = stacked_df[col].map(mapping).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <---- Save purged dataset ---->\n",
    "stacked_df.to_pickle('./data/stacked_purged_recast_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach-1'></a>\n",
    "##### First approach: waves 3, 4 and 5\n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <---- Load purged dataset - CAN SKIP EXECUTION OF ALL PREVIOUS CELLS EXCEPT IMPORT CELL ----> \n",
    "\n",
    "file_path = './data/stacked_purged_recast_df.pickle'\n",
    "stacked_df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospitalized\n",
      "0.0    0.604908\n",
      "1.0    0.395092\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select all rows in wave 5 and all rows from waves 3 and 4 where 'hospitalized' = 1\n",
    "\n",
    "wave_5_df = stacked_df[stacked_df['wave'] == 5]\n",
    "\n",
    "waves_3_4_hosp_df = stacked_df[\n",
    "    (stacked_df['wave'].isin([3, 4])) & (stacked_df['hospitalized'] == 1)\n",
    "]\n",
    "\n",
    "filtered_df = pd.concat([wave_5_df, waves_3_4_hosp_df])\n",
    "\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "hospitalized_counts_filtered_df = filtered_df['hospitalized'].value_counts(normalize=True)\n",
    "print(hospitalized_counts_filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline-no-imputations'></a>\n",
    "##### Baseline: no imputations\n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8001\n",
      "ROC AUC Score: 0.8399\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87      3737\n",
      "           1       0.73      0.52      0.61      1590\n",
      "\n",
      "    accuracy                           0.80      5327\n",
      "   macro avg       0.77      0.72      0.74      5327\n",
      "weighted avg       0.79      0.80      0.79      5327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# 2) Define X and y\n",
    "# ----------------------\n",
    "target_column = 'hospitalized'\n",
    "drop_columns = ['id', 'proxy', 'wave']\n",
    "\n",
    "# --------------------------\n",
    "# 1) Convert to categorical\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "for col, mapping in mappings.items():\n",
    "    if col in stacked_df.columns and col != target_column:\n",
    "        # Extract the numeric portion\n",
    "        stacked_df[col] = stacked_df[col].map(mapping)\n",
    "        \n",
    "        # Convert to actual pandas 'category' dtype\n",
    "        stacked_df[col] = stacked_df[col].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "X = filtered_df.drop(columns=[target_column] + drop_columns)\n",
    "y = filtered_df[target_column].astype('int')  # Keep the target as int\n",
    "\n",
    "# ----------------------\n",
    "# 3) Train/Test Split\n",
    "# ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Initialize XGBoost with enable_categorical\n",
    "# -----------------------------------\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    enable_categorical=True,  \n",
    "    tree_method='hist'        \n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------\n",
    "# 5) Make Predictions\n",
    "# ----------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for ROC AUC\n",
    "\n",
    "# ----------------------\n",
    "# 6) Evaluate\n",
    "# ----------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inspect-hosp-dist'></a>\n",
    "### Baseline: imputations \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <---- Load purged dataset - CAN SKIP EXECUTION OF ALL PREVIOUS CELLS EXCEPT IMPORT CELL ----> \n",
    "\n",
    "file_path = './data/stacked_purged_recast_df.pickle'\n",
    "stacked_df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count for reference: 106537\n",
      "              NaNs count  NaNs Proportion\n",
      "prost              57703         0.541624\n",
      "papsm              50742         0.476285\n",
      "mammog             50227         0.471451\n",
      "breast             50218         0.471367\n",
      "bmi                18201         0.170842\n",
      "rxhibp               807         0.007575\n",
      "rxdiab               796         0.007472\n",
      "diabe                770         0.007228\n",
      "hibpe                735         0.006899\n",
      "cancre               696         0.006533\n",
      "rxresp               694         0.006514\n",
      "respe                676         0.006345\n",
      "rxhrtat              661         0.006204\n",
      "hrtatte              656         0.006157\n",
      "rxstrok              651         0.006111\n",
      "stroke               643         0.006035\n",
      "work                 474         0.004449\n",
      "drinkd               430         0.004036\n",
      "cholst               301         0.002825\n",
      "vigact               209         0.001962\n",
      "doctim1y             169         0.001586\n",
      "doctor1y             169         0.001586\n",
      "mobilseva            135         0.001267\n",
      "adltot6              130         0.001220\n",
      "hltc                  97         0.000910\n",
      "fatigue               89         0.000835\n",
      "swell                 62         0.000582\n",
      "breath_m              60         0.000563\n",
      "painfr                49         0.000460\n",
      "smoken                34         0.000319\n",
      "shlt                  26         0.000244\n",
      "wave                   0         0.000000\n",
      "id                     0         0.000000\n",
      "gender                 0         0.000000\n",
      "hospitalized           0         0.000000\n",
      "proxy                  0         0.000000\n",
      "agey                   0         0.000000\n"
     ]
    }
   ],
   "source": [
    "# <---- Inspect proportions of missing values in selected features ---->\n",
    "missing_data = stacked_df.isna().sum().to_frame(name='NaNs count')\n",
    "missing_data['NaNs Proportion'] = (missing_data['NaNs count'] / len(stacked_df))\n",
    "\n",
    "missing_data = missing_data.sort_values(by='NaNs Proportion', ascending=False)\n",
    "\n",
    "print(f\"Total row count for reference: {stacked_df.shape[0]}\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Proportion of BMI Missing\n",
      "gender                           \n",
      "1.0                      0.113822\n",
      "2.0                      0.221351\n",
      "BMI Missingness by Age Group:\n",
      "age_group\n",
      "50-54      0.141942\n",
      "55-59      0.154147\n",
      "60-64      0.161497\n",
      "65-69      0.167431\n",
      "70-74      0.177162\n",
      "75-79      0.211681\n",
      "80-84      0.249365\n",
      "85-89      0.268004\n",
      "90-94      0.342237\n",
      "95-99      0.440945\n",
      "100-104    0.409091\n",
      "105-109    0.333333\n",
      "110-114    0.000000\n",
      "115-119         NaN\n",
      "Name: bmi, dtype: float64\n",
      "BMI Missingness by Age Group and Gender:\n",
      "gender          1.0       2.0\n",
      "age_group                    \n",
      "50-54      0.084261  0.181320\n",
      "55-59      0.096646  0.200000\n",
      "60-64      0.100380  0.211358\n",
      "65-69      0.108093  0.227247\n",
      "70-74      0.118395  0.240968\n",
      "75-79      0.148237  0.282402\n",
      "80-84      0.190321  0.313466\n",
      "85-89      0.215667  0.327473\n",
      "90-94      0.288401  0.403571\n",
      "95-99      0.312500  0.571429\n",
      "100-104    0.384615  0.444444\n",
      "105-109    0.333333       NaN\n",
      "110-114    0.000000  0.000000\n",
      "115-119         NaN       NaN\n",
      "Median BMI by 5-year Age Groups and Gender:\n",
      "gender           1.0        2.0\n",
      "age_group                      \n",
      "50-54      27.440599  28.194284\n",
      "55-59      27.041643  28.000000\n",
      "60-64      26.755128  27.915520\n",
      "65-69      26.573132  27.407660\n",
      "70-74      26.037493  27.099251\n",
      "75-79      25.469387  26.548130\n",
      "80-84      25.390625  25.390625\n",
      "85-89      24.913494  24.175541\n",
      "90-94      23.309055  24.196493\n",
      "95-99      23.437500  22.959185\n",
      "100-104    22.217696  22.222221\n",
      "105-109    33.243738        NaN\n",
      "110-114    28.326582  27.531231\n",
      "115-119          NaN        NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11689/3088973553.py:44: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bmi_medians = stacked_df.groupby(['age_group', 'gender'])['bmi'].median()\n",
      "/tmp/ipykernel_11689/3088973553.py:48: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_sizes = stacked_df.groupby(['age_group', 'gender'])['bmi'].count()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI Missing Values AFTER Imputation: 0\n",
      "       age_group gender        bmi\n",
      "38974      75-79    2.0  32.444443\n",
      "208704     55-59    2.0  28.000000\n",
      "101661     70-74    2.0  33.163265\n",
      "42584      70-74    1.0  28.303852\n",
      "105160     75-79    2.0  26.562500\n",
      "44147      55-59    1.0  26.851852\n",
      "126969     70-74    1.0  21.203104\n",
      "213018     55-59    2.0  31.980541\n",
      "212991     55-59    1.0  22.230988\n",
      "143628     80-84    1.0  23.388685\n"
     ]
    }
   ],
   "source": [
    "# <---- Imputation of missing values for bmi ---->\n",
    "\n",
    "# Create a mask for missing BMI values\n",
    "bmi_missing_mask = stacked_df['bmi'].isna()\n",
    "\n",
    "# Create a proportion table\n",
    "missing_proportion = stacked_df.groupby('gender', observed=False)['bmi'].apply(lambda x: x.isna().mean())\n",
    "\n",
    "# Print numerical summary\n",
    "print(missing_proportion.to_frame(name=\"Proportion of BMI Missing\"))\n",
    "\n",
    "# Define age bins (every 5 years) from 50 to 115\n",
    "age_bins = list(range(50, 121, 5))  # Covers 50-115+\n",
    "age_labels = [f\"{age_bins[i]}-{age_bins[i+1]-1}\" for i in range(len(age_bins)-1)]\n",
    "\n",
    "# Create a new age group column\n",
    "stacked_df['age_group'] = pd.cut(stacked_df['agey'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Compute BMI missing proportion by age group\n",
    "missing_by_age = stacked_df.groupby('age_group', observed=False)['bmi'].apply(lambda x: x.isna().mean())\n",
    "\n",
    "# Display results\n",
    "print(\"BMI Missingness by Age Group:\")\n",
    "print(missing_by_age)\n",
    "\n",
    "# Group by both age group and gender, then calculate the proportion of missing BMI values\n",
    "missing_bmi_by_age_gender = stacked_df.groupby(['age_group', 'gender'], observed=False)['bmi'].apply(lambda x: x.isna().mean()).unstack()\n",
    "\n",
    "# Display the results\n",
    "print(\"BMI Missingness by Age Group and Gender:\")\n",
    "print(missing_bmi_by_age_gender)\n",
    "\n",
    "# Compute median BMI by 5-year age groups and gender\n",
    "median_bmi_by_age_gender = stacked_df.groupby(['age_group', 'gender'], observed=False)['bmi'].median().unstack()\n",
    "\n",
    "# Display results\n",
    "print(\"Median BMI by 5-year Age Groups and Gender:\")\n",
    "print(median_bmi_by_age_gender)\n",
    "\n",
    "#  Step 1: Ensure 'age_group' column exists\n",
    "age_bins = list(range(50, 121, 5))  # From 50 to 120 in 5-year intervals\n",
    "age_labels = [f\"{i}-{i+4}\" for i in age_bins[:-1]]\n",
    "stacked_df['age_group'] = pd.cut(stacked_df['agey'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "#  Step 2: Compute median BMI for (age_group, gender)\n",
    "bmi_medians = stacked_df.groupby(['age_group', 'gender'])['bmi'].median()\n",
    "\n",
    "#  Step 3: Identify and handle small sample sizes\n",
    "min_sample_size = 10  # Define minimum required count per group\n",
    "group_sizes = stacked_df.groupby(['age_group', 'gender'])['bmi'].count()\n",
    "\n",
    "# If a group has less than 'min_sample_size' observations, merge with the previous group\n",
    "for (age_group, gender), count in group_sizes.items():\n",
    "    if count < min_sample_size:\n",
    "        # Find the previous group\n",
    "        prev_group = str(int(age_group[:2]) - 5) + \"-\" + str(int(age_group[:2]) - 1)\n",
    "        if prev_group in bmi_medians.index.get_level_values(0):\n",
    "            bmi_medians.loc[(age_group, gender)] = bmi_medians.loc[(prev_group, gender)]\n",
    "\n",
    "#  Step 4: Define function to impute missing BMI\n",
    "def impute_bmi(row):\n",
    "    if pd.isna(row['bmi']):  # If BMI is missing\n",
    "        return bmi_medians.get((row['age_group'], row['gender']), np.nan)  # Lookup median value\n",
    "    return row['bmi']  # Keep existing value if not missing\n",
    "\n",
    "#  Step 5: Apply median imputation\n",
    "stacked_df['bmi'] = stacked_df.apply(impute_bmi, axis=1)\n",
    "\n",
    "#  Step 6: Verify results\n",
    "print(\"BMI Missing Values AFTER Imputation:\", stacked_df['bmi'].isna().sum())\n",
    "print(stacked_df[['age_group', 'gender', 'bmi']].sample(10))  # Sample check\n",
    "\n",
    "stacked_df.drop(columns=['age_group'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "### \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8031\n",
      "ROC AUC Score: 0.8403\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87      3737\n",
      "           1       0.73      0.54      0.62      1590\n",
      "\n",
      "    accuracy                           0.80      5327\n",
      "   macro avg       0.78      0.73      0.74      5327\n",
      "weighted avg       0.80      0.80      0.79      5327\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <---- Rerun previous xgboost with imputed bmi ---->\n",
    "\n",
    "# ----------------------\n",
    "# 2) Define X and y\n",
    "# ----------------------\n",
    "target_column = 'hospitalized'\n",
    "drop_columns = ['id', 'proxy', 'wave']\n",
    "\n",
    "# --------------------------\n",
    "# 1) Convert to categorical\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "for col, mapping in mappings.items():\n",
    "    if col in stacked_df.columns and col != target_column:\n",
    "        # Extract the numeric portion\n",
    "        stacked_df[col] = stacked_df[col].map(mapping)\n",
    "        \n",
    "        # Convert to actual pandas 'category' dtype\n",
    "        stacked_df[col] = stacked_df[col].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "X = filtered_df.drop(columns=[target_column] + drop_columns)\n",
    "y = filtered_df[target_column].astype('int')  # Keep the target as int\n",
    "\n",
    "# ----------------------\n",
    "# 3) Train/Test Split\n",
    "# ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Initialize XGBoost with enable_categorical\n",
    "# -----------------------------------\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    enable_categorical=True,  \n",
    "    tree_method='hist'        \n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------\n",
    "# 5) Make Predictions\n",
    "# ----------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for ROC AUC\n",
    "\n",
    "# ----------------------\n",
    "# 6) Evaluate\n",
    "# ----------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <---- Save purged dataset ---->\n",
    "stacked_df.to_pickle('./data/stacked_purged_recast_imputed_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approach-2'></a>\n",
    "### Second approach: all waves\n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### <---- Load purged & imputed dataset ----> \n",
    "\n",
    "file_path = './data/stacked_purged_recast_imputed_df.pickle'\n",
    "stacked_df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospitalized\n",
      "0.0    0.604908\n",
      "1.0    0.395092\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Select all rows in wave 5 and all rows from waves 1, 2, 3 and 4 where 'hospitalized' = 1\n",
    "\n",
    "wave_5_df = stacked_df[stacked_df['wave'] == 5]\n",
    "\n",
    "waves_1_2_3_4_hosp_df = stacked_df[\n",
    "    (stacked_df['wave'].isin([1, 2, 3, 4])) & (stacked_df['hospitalized'] == 1)\n",
    "]\n",
    "\n",
    "filtered_df = pd.concat([wave_5_df, waves_1_2_3_4_hosp_df])\n",
    "\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "hospitalized_counts_filtered_df = filtered_df['hospitalized'].value_counts(normalize=True)\n",
    "print(hospitalized_counts_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7802\n",
      "ROC AUC Score: 0.8415\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      3775\n",
      "           1       0.75      0.64      0.69      2354\n",
      "\n",
      "    accuracy                           0.78      6129\n",
      "   macro avg       0.77      0.75      0.76      6129\n",
      "weighted avg       0.78      0.78      0.78      6129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <---- Run xgboost with all waves ---->\n",
    "\n",
    "# ----------------------\n",
    "# 2) Define X and y\n",
    "# ----------------------\n",
    "target_column = 'hospitalized'\n",
    "drop_columns = ['id', 'proxy', 'wave']\n",
    "\n",
    "# --------------------------\n",
    "# 1) Convert to categorical\n",
    "# --------------------------\n",
    "# Instead of .astype('float64'), switch to .astype('category') \n",
    "# after extracting the numeric portion from your string categories.\n",
    "\n",
    "for col, mapping in mappings.items():\n",
    "    if col in stacked_df.columns and col != target_column:\n",
    "        # Extract the numeric portion\n",
    "        stacked_df[col] = stacked_df[col].map(mapping)\n",
    "        \n",
    "        # Convert to actual pandas 'category' dtype\n",
    "        stacked_df[col] = stacked_df[col].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "X = filtered_df.drop(columns=[target_column] + drop_columns)\n",
    "y = filtered_df[target_column].astype('int')  # Keep the target as int\n",
    "\n",
    "# ----------------------\n",
    "# 3) Train/Test Split\n",
    "# ----------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------\n",
    "# 4) Initialize XGBoost with enable_categorical\n",
    "# -----------------------------------\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    enable_categorical=True,  # <--- Tells XGBoost to handle categorical splits\n",
    "    tree_method='hist'        # or 'gpu_hist'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------\n",
    "# 5) Make Predictions\n",
    "# ----------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability for ROC AUC\n",
    "\n",
    "# ----------------------\n",
    "# 6) Evaluate\n",
    "# ----------------------\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature  Importance\n",
      "11   doctim1y    0.069782\n",
      "1        shlt    0.058321\n",
      "21    fatigue    0.052362\n",
      "10   doctor1y    0.049721\n",
      "29    rxhrtat    0.045892\n",
      "12     cholst    0.044773\n",
      "4   mobilseva    0.042787\n",
      "22      swell    0.038335\n",
      "15      papsm    0.036367\n",
      "6     hrtatte    0.033358\n",
      "14     mammog    0.032489\n",
      "3     adltot6    0.032237\n",
      "2        hltc    0.029073\n",
      "18     drinkd    0.026333\n",
      "30    rxstrok    0.026296\n",
      "31       work    0.026251\n",
      "26     rxresp    0.024965\n",
      "23      hibpe    0.024803\n",
      "24     cancre    0.024520\n",
      "28     rxdiab    0.023064\n",
      "25      respe    0.022942\n",
      "9         bmi    0.022780\n",
      "16      prost    0.022231\n",
      "20     painfr    0.021277\n",
      "13     breast    0.021158\n",
      "32       agey    0.020684\n",
      "27     rxhibp    0.019580\n",
      "19     smoken    0.018915\n",
      "17     vigact    0.018639\n",
      "5       diabe    0.018595\n",
      "7      stroke    0.018438\n",
      "8    breath_m    0.016872\n",
      "0      gender    0.016162\n"
     ]
    }
   ],
   "source": [
    "# Extract feature importance\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance values\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "### \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "### \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "### \n",
    "\n",
    "<small>[Back to top](#sections)</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
